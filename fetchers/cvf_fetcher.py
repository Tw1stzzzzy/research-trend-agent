import requests
from bs4 import BeautifulSoup
import time

class CVFFetcher:
    """
    çˆ¬å– CVF ä¼šè®®ï¼ˆCVPR, ICCV, ECCVï¼‰å…¬å¼€è®ºæ–‡åˆ—è¡¨ã€‚
    ä»¥ CVPR2023 ä¸ºä¾‹ï¼ŒURL: https://openaccess.thecvf.com/CVPR2023
    """

    def __init__(self, conference_url, venue_name):
        """
        conference_url: ä¾‹å¦‚ "https://openaccess.thecvf.com/CVPR2023"
        venue_name: ä¾‹å¦‚ "CVPR"
        """
        self.base_url = conference_url
        self.venue = venue_name

    def fetch_papers(self, max_papers=200):
        """
        è·å–CVFä¼šè®®çš„å·²å‘è¡¨è®ºæ–‡
        max_papers: é™åˆ¶è·å–çš„è®ºæ–‡æ•°é‡ï¼Œé¿å…å¤„ç†æ—¶é—´è¿‡é•¿
        """
        print(f"ğŸ” Fetching papers from {self.venue} ({self.base_url})...")
        
        # æ„å»ºè·å–æ‰€æœ‰è®ºæ–‡çš„URL
        all_papers_url = f"{self.base_url}?day=all"
        
        try:
            response = requests.get(all_papers_url, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # æŸ¥æ‰¾è®ºæ–‡æ ‡é¢˜ï¼ˆåœ¨dtæ ‡ç­¾ä¸­ï¼‰
            title_tags = soup.find_all('dt')
            print(f"ğŸ“‹ Found {len(title_tags)} papers, processing up to {max_papers}...")

            papers = []
            processed_count = 0
            
            for i, dt_tag in enumerate(title_tags):
                if processed_count >= max_papers:
                    print(f"â¹ï¸  Reached limit of {max_papers} papers")
                    break
                
                title = dt_tag.text.strip()
                if not title:
                    continue
                
                # æŸ¥æ‰¾å¯¹åº”çš„è®ºæ–‡è¯¦æƒ…ï¼ˆé€šå¸¸åœ¨ä¸‹ä¸€ä¸ªddæ ‡ç­¾ä¸­ï¼‰
                dd_tag = dt_tag.find_next_sibling('dd')
                authors = []
                abstract = ""
                pdf_url = ""
                
                if dd_tag:
                    # æå–ä½œè€…ä¿¡æ¯
                    author_links = dd_tag.find_all('a')
                    authors = [a.text.strip() for a in author_links if a.text.strip() and not a.text.strip().startswith('http')]
                    
                    # æŸ¥æ‰¾PDFé“¾æ¥
                    pdf_link = dd_tag.find('a', href=True)
                    if pdf_link and pdf_link.get('href'):
                        pdf_url = pdf_link.get('href')
                        # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œè½¬æ¢ä¸ºç»å¯¹è·¯å¾„
                        if pdf_url.startswith('/'):
                            pdf_url = f"https://openaccess.thecvf.com{pdf_url}"

                paper_data = {
                    'title': title,
                    'authors': authors[:5],  # é™åˆ¶ä½œè€…æ•°é‡
                    'abstract': abstract,  # CVFé¡µé¢é€šå¸¸ä¸åŒ…å«æ‘˜è¦
                    'pdf_url': pdf_url,
                    'venue': self.venue,
                    'decision': 'Published (CVF Open Access)'
                }
                
                papers.append(paper_data)
                processed_count += 1
                
                # è¿›åº¦æç¤º
                if (processed_count) % 50 == 0:
                    print(f"  â³ Processed {processed_count} papers...")
                    time.sleep(0.1)  # å°å»¶è¿Ÿé¿å…è¿‡å¿«å¤„ç†

            print(f"âœ… Successfully fetched {len(papers)} papers from {self.venue}")
            return papers

        except requests.RequestException as e:
            print(f"âŒ Error fetching from {all_papers_url}: {e}")
            return []
        except Exception as e:
            print(f"âŒ Error parsing CVF papers: {e}")
            return []

    def get_paper_abstract(self, paper_url):
        """
        è·å–å•ç¯‡è®ºæ–‡çš„æ‘˜è¦ï¼ˆå¦‚æœæœ‰è¯¦æƒ…é¡µï¼‰
        """
        try:
            response = requests.get(paper_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾æ‘˜è¦
            abstract_tag = soup.find('div', {'id': 'abstract'}) or soup.find('div', class_='abstract')
            if abstract_tag:
                return abstract_tag.get_text().strip()
            
            return ""
        except Exception:
            return ""
