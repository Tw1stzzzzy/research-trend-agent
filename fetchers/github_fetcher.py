import requests
import re
import time
import yaml
from datetime import datetime

class GitHubFetcher:
    """
    åˆ©ç”¨ GitHub API æŸ¥è¯¢æŸä¸ª Repo çš„ stars æ•°é‡ä¸å¼€æºæ—¶é•¿ï¼ˆå¤©ï¼‰ã€‚
    """

    def __init__(self, github_token):
        self.headers = {"Authorization": f"token {github_token}"}
        # ä»é…ç½®æ–‡ä»¶åŠ è½½é»‘åå•
        self.repo_blacklist = {}
        try:
            with open("configs/config.yaml", "r") as f:
                config = yaml.safe_load(f)
                self.repo_blacklist = config.get('repo_blacklist', {})
                if self.repo_blacklist:
                    print(f"ğŸ“‹ Loaded repository blacklist with {len(self.repo_blacklist)} entries")
        except Exception as e:
            print(f"âš ï¸  Warning: Could not load blacklist from config: {e}")

    def get_repo_stats(self, repo_url):
        """
        repo_url å½¢å¦‚ "https://github.com/owner/repo"
        è¿”å›å­—å…¸ï¼š{'stars': int, 'forks': int, 'days_since_created': int}
        è‹¥æ— æ³•è·å–åˆ™è¿”å› None
        """
        if not repo_url or 'github.com' not in repo_url:
            return None

        owner_repo = repo_url.replace("https://github.com/", "").strip("/")
        api_url = f"https://api.github.com/repos/{owner_repo}"
        response = requests.get(api_url, headers=self.headers)
        if response.status_code != 200:
            return None

        data = response.json()
        created_at = data.get('created_at')
        stars = data.get('stargazers_count', 0)
        forks = data.get('forks_count', 0)

        created_date = datetime.strptime(created_at, "%Y-%m-%dT%H:%M:%SZ")
        days_since_created = (datetime.now() - created_date).days

        return {
            'stars': stars,
            'forks': forks,
            'days_since_created': days_since_created
        }
    
    def extract_keywords(self, title):
        """
        ä»è®ºæ–‡æ ‡é¢˜ä¸­æå–å…³é”®è¯ï¼Œç”¨äºGitHubæœç´¢
        æ”¹è¿›ç‰ˆï¼šæ›´ç²¾ç¡®åœ°æå–æ¨¡å‹åç§°å’ŒæŠ€æœ¯è¯æ±‡
        """
        # ç§»é™¤å¸¸è§åœç”¨è¯
        stopwords = {'via', 'for', 'with', 'on', 'in', 'the', 'a', 'an', 'and', 'or', 'but', 'of', 'to', 'from', 'by', 'at', 'using', 'towards', 'how', 'your'}
        
        # é¢„å¤„ç†æ ‡é¢˜ï¼Œå¤„ç†ç‰¹æ®Šå­—ç¬¦
        cleaned_title = re.sub(r'[^\w\s\-:]', ' ', title)
        
        # 1. æå–å¯èƒ½çš„æ¨¡å‹/æ–¹æ³•åç§°ï¼ˆé€šå¸¸æ˜¯å†’å·å‰çš„éƒ¨åˆ†ï¼‰
        main_concept = cleaned_title.split(':')[0].strip() if ':' in cleaned_title else ''
        model_words = []
        if main_concept:
            # æå–æ¨¡å‹åä¸­çš„å•è¯ï¼ˆé€šå¸¸æ˜¯å¤§å†™å­—æ¯å¼€å¤´æˆ–å…¨å¤§å†™ï¼‰
            model_words = re.findall(r'\b[A-Z][a-zA-Z0-9]*(?:-[A-Za-z0-9]+)*\b|\b[A-Z]{2,}\b', main_concept)
        
        # 2. æå–ç‰¹æ®Šæ ¼å¼çš„æ¨¡å‹åï¼ˆå¦‚ViT, GPT, BERTç­‰å…¨å¤§å†™ç¼©å†™ï¼‰
        special_models = re.findall(r'\b[A-Z]{2,}(?:-[A-Z]+)*\b', title)
        
        # 3. è¯†åˆ«æ··åˆå¤§å°å†™çš„æ¨¡å‹åï¼ˆå¦‚BiGAN, DeepDreamç­‰ï¼‰
        mixed_case = re.findall(r'\b[A-Z][a-z]+[A-Z][a-zA-Z]*\b', title)
        
        # 4. æå–åŒ…å«æ•°å­—å’Œè¿å­—ç¬¦çš„æŠ€æœ¯è¯æ±‡ï¼ˆå¦‚3Dã€X-Rayç­‰ï¼‰
        tech_terms = re.findall(r'\b\d+[A-Za-z]+\b|\b[A-Za-z]+\d+[A-Za-z]*\b|\b[A-Za-z]+[-][A-Za-z]+\b', title)
        
        # 5. æå–é‡è¦çš„é¢†åŸŸè¯æ±‡ï¼ˆé€šå¸¸æ˜¯åè¯çŸ­è¯­ï¼‰
        words = re.findall(r'\b[A-Za-z]+(?:-[A-Za-z]+)*\b', title)
        keywords = [word for word in words if word.lower() not in stopwords and len(word) > 2]
        
        # 6. æå–å®Œæ•´çš„çŸ­è¯­ç»„åˆï¼ˆé€šå¸¸æ›´å…·ä»£è¡¨æ€§ï¼‰
        if ':' in title:
            main_part = title.split(':')[0].strip()
            if 2 < len(main_part.split()) < 5:  # åˆç†é•¿åº¦çš„çŸ­è¯­
                if all(w.lower() not in stopwords for w in main_part.split()):
                    model_words.append(main_part)
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯ï¼Œä¼˜å…ˆçº§ï¼šæ¨¡å‹å > ç‰¹æ®Šæ¨¡å‹ > æ··åˆå¤§å°å†™ > æŠ€æœ¯è¯æ±‡ > é¢†åŸŸè¯æ±‡
        all_keywords = model_words + special_models + mixed_case + tech_terms + keywords
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        unique_keywords = []
        seen = set()
        for kw in all_keywords:
            if kw.lower() not in seen:
                unique_keywords.append(kw)
                seen.add(kw.lower())
        
        # å¯¹å…³é”®è¯è¿›è¡Œé¢å¤–è¿‡æ»¤ï¼Œæ’é™¤å¸¸è§çš„è¯¯å¯¼è¯
        filtered_keywords = [k for k in unique_keywords if k.lower() not in {
            'how', 'why', 'what', 'when', 'where', 'which', 'who', 'whose', 'whom', 
            'your', 'our', 'their', 'model', 'models', 'method', 'methods', 
            'approach', 'approaches', 'robust', 'novel', 'new', 'paper'
        }]
        
        # å¦‚æœè¿‡æ»¤åæ²¡æœ‰å…³é”®è¯ï¼Œè¿”å›åŸå§‹å…³é”®è¯ï¼ˆé¿å…è¿‡åº¦è¿‡æ»¤ï¼‰
        if not filtered_keywords and unique_keywords:
            return unique_keywords[:6]
            
        return filtered_keywords[:6]  # è¿”å›å‰6ä¸ªæœ€é‡è¦çš„å…³é”®è¯
    
    def search_paper_repository(self, paper_title):
        """
        åŸºäºè®ºæ–‡æ ‡é¢˜æœç´¢GitHubä»“åº“
        è¿”å›æœ€åŒ¹é…çš„ä»“åº“URLå’Œç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æœæ²¡æ‰¾åˆ°è¿”å›None
        æ”¹è¿›ç‰ˆï¼šä½¿ç”¨å¤šé‡æœç´¢ç­–ç•¥å’Œä¸¥æ ¼éªŒè¯
        """
        print(f"    ğŸ” Searching GitHub for: {paper_title[:50]}...")
        
        # æ£€æŸ¥å½“å‰è®ºæ–‡æ˜¯å¦åœ¨é»‘åå•ä¸­
        self._paper_specific_blacklist = []
        for blacklist_title, blacklist_repos in self.repo_blacklist.items():
            if blacklist_title in paper_title:
                print(f"    âš ï¸  Special case: '{blacklist_title}' has known bad matches")
                # åœ¨éªŒè¯æ—¶ä¼šè¿‡æ»¤è¿™äº›ä»“åº“
                self._paper_specific_blacklist = blacklist_repos
                break
        
        # æå–å…³é”®è¯ï¼Œå»é™¤å¸¸è§é—®é¢˜è¯
        keywords = self.extract_keywords(paper_title)
        if not keywords:
            print(f"    âŒ No valid keywords extracted")
            return None
        
        # è¿‡æ»¤æ‰é—®é¢˜æ€§å…³é”®è¯ï¼ˆtoo general or misleadingï¼‰
        keywords = [k for k in keywords if k.lower() not in {'how', 'why', 'what', 'when', 'where', 'which', 'who', 'whose', 'whom', 'your', 'our', 'their', 'model', 'models', 'method', 'methods', 'approach', 'approaches'}]
        
        if not keywords:
            print(f"    âŒ No specific keywords available after filtering")
            return None
            
        print(f"    ğŸ”‘ Keywords: {keywords[:3]}")
        
        # ç­–ç•¥1: å°è¯•ç²¾ç¡®çš„æ¨¡å‹åæœç´¢ (ä½¿ç”¨å¼•å·åŒ…å›´å®Œæ•´çš„æ¨¡å‹å)
        if keywords and len(keywords[0]) > 2:
            if re.match(r'^[A-Z]', keywords[0]) or '-' in keywords[0]:  # åªæœ‰æ¨¡å‹åæ‰ç”¨ç²¾ç¡®åŒ¹é…
                result = self._search_with_exact_match(keywords[0], paper_title, keywords)
                if result:
                    return result
        
        # ç­–ç•¥2: ç»„åˆå‰ä¸¤ä¸ªå…³é”®è¯æœç´¢ + å…·ä½“åŒ–
        if len(keywords) >= 2:
            # æ·»åŠ ç‰¹å®šå…³é”®è¯ï¼Œæé«˜æœç´¢ç²¾ç¡®åº¦
            domain_keywords = ['paper', 'implementation', 'official']
            
            # å°è¯•åŒ…å«ç‰¹å®šå…³é”®è¯çš„æœç´¢
            for domain_keyword in domain_keywords:
                if len(keywords) >= 2:
                    result = self._search_with_specific_context(keywords[:2], domain_keyword, paper_title, keywords)
                    if result:
                        return result
            
            # å¸¸è§„å¤šå…³é”®è¯æœç´¢
            result = self._search_with_multiple_keywords(keywords[:2], paper_title, keywords)
            if result:
                return result
        
        # ç­–ç•¥3: å•ä¸ªæœ€é‡è¦å…³é”®è¯æœç´¢ + å…·ä½“åŒ–
        domain_keywords = ['paper', 'implementation', 'official', 'code']
        for domain_keyword in domain_keywords:
            result = self._search_with_specific_context([keywords[0]], domain_keyword, paper_title, keywords)
            if result:
                return result
        
        # ç­–ç•¥4: æœ€åçš„å•å…³é”®è¯æœç´¢
        result = self._search_with_single_keyword(keywords[0], paper_title, keywords)
        if result:
            return result
        
        print(f"    âŒ No matching repository found with strict criteria")
        return None
    
    def _search_with_exact_match(self, keyword, paper_title, all_keywords):
        """ä½¿ç”¨ç²¾ç¡®åŒ¹é…æœç´¢ç­–ç•¥"""
        query = f'"{keyword}" language:python'
        print(f"    ğŸ¯ Exact match search: {query}")
        return self._search_github_repos(query, paper_title, all_keywords, strategy="exact")
    
    def _search_with_multiple_keywords(self, keywords, paper_title, all_keywords):
        """ä½¿ç”¨å¤šå…³é”®è¯æœç´¢ç­–ç•¥"""
        query = f"{' '.join(keywords)} language:python"
        print(f"    ğŸ” Multi-keyword search: {query}")
        return self._search_github_repos(query, paper_title, all_keywords, strategy="multi")
    
    def _search_with_single_keyword(self, keyword, paper_title, all_keywords):
        """ä½¿ç”¨å•å…³é”®è¯æœç´¢ç­–ç•¥"""
        query = f"{keyword} language:python"
        print(f"    ğŸ” Single keyword search: {query}")
        return self._search_github_repos(query, paper_title, all_keywords, strategy="single")
    
    def _search_with_specific_context(self, keywords, context_keyword, paper_title, all_keywords):
        """ä½¿ç”¨ç‰¹å®šä¸Šä¸‹æ–‡çš„æœç´¢ç­–ç•¥"""
        query = f"{' '.join(keywords)} {context_keyword} language:python"
        print(f"    ğŸ”¬ Contextual search: {query}")
        return self._search_github_repos(query, paper_title, all_keywords, strategy="context")
    
    def _search_github_repos(self, query, paper_title, keywords, strategy="general"):
        """
        æ‰§è¡ŒGitHubæœç´¢å¹¶è¿”å›æœ€ä½³åŒ¹é…
        """
        api_url = "https://api.github.com/search/repositories"
        params = {
            'q': query,
            'sort': 'stars',
            'order': 'desc',
            'per_page': 15  # å¢åŠ æœç´¢ç»“æœæ•°é‡
        }
        
        try:
            response = requests.get(api_url, headers=self.headers, params=params)
            
            if response.status_code == 403:  # Rate limit
                print(f"    âš ï¸  GitHub API rate limit, waiting...")
                time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
                response = requests.get(api_url, headers=self.headers, params=params)
            
            if response.status_code != 200:
                print(f"    âŒ GitHub search failed: {response.status_code}")
                return None
            
            data = response.json()
            if not data.get('items'):
                print(f"    âŒ No repositories found")
                return None
            
            # å¯¹ç»“æœè¿›è¡Œç›¸å…³æ€§æ’åº
            best_repo = self._rank_repositories(data['items'], paper_title, keywords, strategy)
            
            if best_repo:
                repo_url = best_repo['html_url']
                
                # é¢å¤–éªŒè¯ï¼šæ£€æŸ¥ä»“åº“å†…å®¹æ˜¯å¦çœŸçš„ä¸è®ºæ–‡ç›¸å…³
                if self._verify_repository_relevance(repo_url, paper_title, keywords):
                    stats = self.get_repo_stats(repo_url)
                    if stats:
                        print(f"    âœ… Found: {best_repo['name']} ({stats['stars']} â­)")
                        return {
                            'repo_url': repo_url,
                            'stats': stats
                        }
                else:
                    print(f"    âŒ Repository verification failed: not relevant to paper")
            
            return None
        
        except Exception as e:
            print(f"    âŒ GitHub search error: {e}")
        
        return None
    
    def _verify_repository_relevance(self, repo_url, paper_title, keywords):
        """
        éªŒè¯ä»“åº“ä¸è®ºæ–‡çš„ç›¸å…³æ€§
        æ£€æŸ¥READMEå†…å®¹ã€ä»“åº“æè¿°ç­‰
        """
        try:
            # è·å–ä»“åº“ä¿¡æ¯
            owner_repo = repo_url.replace("https://github.com/", "").strip("/")
            api_url = f"https://api.github.com/repos/{owner_repo}"
            response = requests.get(api_url, headers=self.headers)
            
            if response.status_code != 200:
                return True  # å¦‚æœæ— æ³•è·å–ä¿¡æ¯ï¼Œä¸æ‹’ç»
            
            repo_data = response.json()
            repo_name = repo_data.get('name', '').lower()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨å½“å‰è®ºæ–‡çš„ç‰¹å®šé»‘åå•ä¸­
            if hasattr(self, '_paper_specific_blacklist') and self._paper_specific_blacklist:
                if repo_name in [r.lower() for r in self._paper_specific_blacklist]:
                    print(f"    âš ï¸  Repository {repo_name} is blacklisted for this paper")
                    return False
            
            description = repo_data.get('description', '') or ''
            
            # ç‰¹åˆ«è¿‡æ»¤æ‰ä¸€äº›æ˜æ˜¾ä¸ç›¸å…³çš„é€šç”¨å·¥å…·åº“ (æ‰©å±•åˆ—è¡¨)
            known_tools = ['howdoi', 'graphrag', 'xpra', 'wavelets', 'nboost', 'awesome-pytorch-papers', 
                          'awesome-deep-learning', 'awesome-machine-learning', 'pytorch-tutorial', 
                          'tensorflow-examples', 'robustmatting', 'robustvideo']
            if repo_name in known_tools or any(kt in repo_name for kt in known_tools):
                if not any(kw.lower() in repo_name.replace('-', ' ').replace('_', ' ') for kw in keywords[:2]):
                    print(f"    âš ï¸  Filtering out known tool library: {repo_name}")
                    return False
                
            # è¿‡æ»¤ä¸€äº›æ˜æ˜¾ä¸ç›¸å…³çš„å·¥å…·ç±»å‹
            tool_patterns = ['cli', 'tool', 'utility', 'framework', 'awesome', 'list', 'collection', 
                            'tutorial', 'example', 'template', 'boilerplate', 'starter', 'kit']
            if any(pattern in repo_name for pattern in tool_patterns) and not any(kw.lower() in repo_name for kw in keywords[:2]):
                # æœ‰å·¥å…·ç‰¹å¾ï¼Œä½†æ²¡æœ‰è®ºæ–‡å…³é”®è¯ç‰¹å¾
                stars = repo_data.get('stargazers_count', 0)
                if stars > 1000:  # é«˜æ˜Ÿé€šç”¨å·¥å…·å¾ˆå¯èƒ½ä¸æ˜¯è®ºæ–‡å®ç°
                    print(f"    âš ï¸  Filtering out high-star general tool: {repo_name} ({stars} stars)")
                    return False
            
            # ä¸¥æ ¼æ£€æŸ¥ä»“åº“åä¸è®ºæ–‡å…³é”®è¯çš„åŒ¹é…åº¦
            # æå–è®ºæ–‡æ ‡é¢˜ä¸­çš„å…³é”®è¯ç‰¹å¾ï¼ˆæ›´ç»†è‡´åœ°å¤„ç†ï¼‰
            paper_title_lower = paper_title.lower()
            
            # 1. æå–ä¸»è¦æ¦‚å¿µï¼ˆå†’å·å‰çš„éƒ¨åˆ†æˆ–ç¬¬ä¸€ä¸ªè¯ç»„ï¼‰
            primary_concept = ""
            if ':' in paper_title:
                primary_concept = paper_title.split(':')[0].strip().lower()
            else:
                # å–å‰3ä¸ªè¯ä½œä¸ºä¸»è¦æ¦‚å¿µ
                words = paper_title.split()
                primary_concept = ' '.join(words[:min(3, len(words))]).lower()
            
            # 2. æ£€æŸ¥ä»“åº“åæ˜¯å¦åŒ…å«ä¸»è¦æ¦‚å¿µä¸­çš„å…³é”®è¯
            primary_concept_words = re.findall(r'\b[a-z0-9]{3,}\b', primary_concept)
            repo_name_words = re.findall(r'[a-z0-9]+', repo_name)
            
            # å¦‚æœä¸åŒ…å«ä»»ä½•ä¸»è¦æ¦‚å¿µè¯ï¼Œå¯èƒ½ä¸ç›¸å…³
            if primary_concept_words and not any(word in repo_name for word in primary_concept_words):
                # ä½†å¦‚æœæ˜¯ä½æ˜Ÿä»“åº“ï¼Œæˆ‘ä»¬æ›´å®½æ¾ä¸€äº›
                stars = repo_data.get('stargazers_count', 0)
                if stars > 500:
                    # å¦‚æœæ˜¯é«˜æ˜Ÿä»“åº“ï¼Œä½†åç§°ä¸è®ºæ–‡ä¸»è¦æ¦‚å¿µæ— å…³ï¼Œå¯èƒ½ä¸ç›¸å…³
                    # éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼è¿›ä¸€æ­¥éªŒè¯
                    strict_verification_needed = True
                else:
                    # ä½æ˜Ÿä»“åº“æ›´å®½æ¾ä¸€äº›
                    strict_verification_needed = False
            else:
                # æœ‰ä¸»è¦æ¦‚å¿µè¯åŒ¹é…ï¼Œå¯èƒ½ç›¸å…³
                strict_verification_needed = False
            
            # æ£€æŸ¥æè¿°ä¸­æ˜¯å¦åŒ…å«å…³é”®è¯
            description_lower = description.lower()
            
            # é¦–å…ˆï¼Œæ£€æŸ¥æè¿°ä¸­æ˜¯å¦ç›´æ¥æåˆ°è®ºæ–‡
            paper_indicators = ['paper', 'implementation', 'code', 'official', 'reproduction', 'pytorch']
            if any(indicator in description_lower for indicator in paper_indicators):
                # æè¿°ä¸­æåˆ°è®ºæ–‡ç›¸å…³è¯æ±‡ï¼Œä¸”åŒ…å«å…³é”®è¯
                core_words = [kw.lower() for kw in keywords[:3]]
                if any(word in description_lower for word in core_words):
                    print(f"    âœ… Description mentions paper implementation")
                    return True
            
            # å¦‚æœæè¿°ä¸­åŒ…å«è®ºæ–‡çš„æ ¸å¿ƒè¯æ±‡ï¼Œè®¤ä¸ºç›¸å…³
            core_words = [kw.lower() for kw in keywords[:3]]
            matches = sum(1 for word in core_words if word in description_lower)
            
            if matches >= 1:  # è‡³å°‘åŒ¹é…ä¸€ä¸ªæ ¸å¿ƒè¯
                # ä½†è¦æ’é™¤è¿‡äºé€šç”¨çš„æè¿°
                if len(description_lower) < 15 or description_lower in ['deep learning', 'machine learning', 'artificial intelligence']:
                    # è¿‡äºç®€çŸ­æˆ–é€šç”¨çš„æè¿°ä¸è¶³ä»¥åˆ¤æ–­ç›¸å…³æ€§
                    pass
                else:
                    return True
            
            # å¯¹äºéœ€è¦ä¸¥æ ¼éªŒè¯çš„ä»“åº“ï¼Œæ£€æŸ¥READMEå†…å®¹
            if strict_verification_needed:
                # è·å–READMEå†…å®¹
                readme_url = f"https://api.github.com/repos/{owner_repo}/readme"
                readme_response = requests.get(readme_url, headers=self.headers)
                
                if readme_response.status_code == 200:
                    readme_data = readme_response.json()
                    if 'content' in readme_data:
                        import base64
                        readme_content = base64.b64decode(readme_data['content']).decode('utf-8', errors='ignore').lower()
                        
                        # 1. æ£€æŸ¥READMEä¸­æ˜¯å¦ç›´æ¥æåˆ°è®ºæ–‡æ ‡é¢˜
                        paper_title_clean = re.sub(r'[^\w\s]', ' ', paper_title.lower())
                        title_words = set(paper_title_clean.split())
                        title_word_count = len(title_words)
                        
                        matches = sum(1 for word in title_words if word.lower() in readme_content)
                        match_ratio = matches / title_word_count if title_word_count > 0 else 0
                        
                        # READMEåŒ…å«è¶…è¿‡50%çš„æ ‡é¢˜è¯ï¼Œå¯èƒ½ç›¸å…³
                        if match_ratio > 0.5:
                            print(f"    âœ… README contains many paper title words: {match_ratio:.1%}")
                            return True
                        
                        # 2. æ£€æŸ¥READMEä¸­æ˜¯å¦æœ‰è®ºæ–‡çš„å…³é”®è¯
                        important_words = [word for word in title_words if len(word) > 3 and word not in {'with', 'using', 'for', 'from', 'the', 'and'}]
                        
                        readme_matches = sum(1 for word in important_words if word in readme_content)
                        if readme_matches >= 2:  # READMEä¸­è‡³å°‘å‡ºç°2ä¸ªé‡è¦è¯æ±‡
                            print(f"    âœ… README contains multiple paper keywords")
                            return True
                        
                        # 3. æ£€æŸ¥READMEä¸­æ˜¯å¦æåˆ°arXivæˆ–è®ºæ–‡å¼•ç”¨
                        if 'arxiv' in readme_content or 'paper' in readme_content:
                            # åœ¨arxivæˆ–paperæåŠé™„è¿‘æ£€æŸ¥æ˜¯å¦æœ‰è®ºæ–‡å…³é”®è¯
                            arxiv_idx = readme_content.find('arxiv')
                            if arxiv_idx == -1:
                                arxiv_idx = readme_content.find('paper')
                            
                            if arxiv_idx != -1:
                                # æ£€æŸ¥å‘¨å›´ä¸Šä¸‹æ–‡
                                context_window = readme_content[max(0, arxiv_idx-200):min(len(readme_content), arxiv_idx+200)]
                                
                                # è®¡ç®—ä¸Šä¸‹æ–‡ä¸­æœ‰å¤šå°‘è®ºæ–‡å…³é”®è¯
                                context_matches = sum(1 for word in important_words if word in context_window)
                                if context_matches >= 1:
                                    print(f"    âœ… README mentions arxiv/paper with relevant keywords")
                                    return True
                
                # ä¸¥æ ¼éªŒè¯å¤±è´¥ï¼Œä»“åº“å¯èƒ½ä¸ç›¸å…³
                print(f"    âŒ Repository fails strict verification: likely not related to paper")
                return False
            
            # æå–è®ºæ–‡æ ‡é¢˜çš„ç‰¹å¾éƒ¨åˆ†ï¼ˆé€šå¸¸æ˜¯å†’å·å‰çš„éƒ¨åˆ†ï¼Œå¦‚"BiFormer: Vision Transformer..."ä¸­çš„"BiFormer"ï¼‰
            title_prefix = paper_title.split(':')[0].strip() if ':' in paper_title else paper_title.split()[0]
            
            # è·å–READMEå†…å®¹
            readme_url = f"https://api.github.com/repos/{owner_repo}/readme"
            readme_response = requests.get(readme_url, headers=self.headers)
            
            if readme_response.status_code == 200:
                readme_data = readme_response.json()
                if 'content' in readme_data:
                    import base64
                    readme_content = base64.b64decode(readme_data['content']).decode('utf-8', errors='ignore').lower()
                    
                    # 1. æ£€æŸ¥READMEä¸­æ˜¯å¦ç›´æ¥æåˆ°è®ºæ–‡æ ‡é¢˜
                    title_words = set(paper_title.lower().split())
                    
                    # 2. æ£€æŸ¥READMEä¸­æ˜¯å¦æœ‰è®ºæ–‡çš„å…³é”®è¯
                    important_words = [word for word in title_words if len(word) > 3 and word not in {'with', 'using', 'for', 'from', 'the', 'and'}]
                    
                    readme_matches = sum(1 for word in important_words if word in readme_content)
                    if readme_matches >= 2:  # READMEä¸­è‡³å°‘å‡ºç°2ä¸ªé‡è¦è¯æ±‡
                        print(f"    âœ… README contains multiple paper keywords")
                        return True
                        
                    # 3. æ£€æŸ¥READMEä¸­æ˜¯å¦åŒ…å«è®ºæ–‡æ ‡é¢˜æˆ–ç‰¹å¾éƒ¨åˆ†
                    if title_prefix.lower() in readme_content:
                        # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦ä¸è®ºæ–‡ç›¸å…³
                        title_idx = readme_content.find(title_prefix.lower())
                        context_window = readme_content[max(0, title_idx-50):min(len(readme_content), title_idx+50)]
                        if any(kw in context_window for kw in ['paper', 'implementation', 'code', 'official', 'arxiv']):
                            print(f"    âœ… README mentions paper title in relevant context")
                            return True
                    
                    # 4. æ£€æŸ¥æ˜¯å¦æœ‰arXivæˆ–DOIé“¾æ¥
                    if 'arxiv.org' in readme_content or 'doi.org' in readme_content:
                        # æœ‰å­¦æœ¯å¼•ç”¨é“¾æ¥ï¼Œä¸”è‡³å°‘æœ‰ä¸€ä¸ªå…³é”®è¯åŒ¹é…
                        if any(word in readme_content for word in important_words):
                            print(f"    âœ… README contains academic references and paper keywords")
                            return True
            
            # å¦‚æœæè¿°å’ŒREADMEéƒ½æ²¡æœ‰è¶³å¤Ÿçš„åŒ¹é…ï¼Œä½†ä»“åº“åç›´æ¥åŒ…å«å…³é”®è¯ï¼Œä¹Ÿè®¤ä¸ºç›¸å…³
            for keyword in core_words:
                if keyword.lower() in repo_name:
                    print(f"    âœ… Repository name contains key model term: {keyword}")
                    return True
            
            # å¯¹äºä½æ˜Ÿä»“åº“æ›´å®½æ¾ä¸€äº›
            stars = repo_data.get('stargazers_count', 0)
            if stars < 100:
                # å¯¹äºä½æ˜Ÿä»“åº“ï¼Œå¦‚æœä»“åº“åæˆ–æè¿°ä¸­æœ‰ä»»ä½•ä¸€ä¸ªå…³é”®è¯åŒ¹é…ï¼Œéƒ½æ¥å—
                for kw in keywords:
                    if kw.lower() in repo_name or kw.lower() in description_lower:
                        print(f"    âœ… Low-star repo with keyword match: {kw}")
                        return True
            
            return False  # éªŒè¯å¤±è´¥
            
        except Exception as e:
            print(f"    âš ï¸  Verification error: {e}")
            return True  # éªŒè¯å‡ºé”™æ—¶ä¸æ‹’ç»ï¼Œé¿å…è¿‡äºä¸¥æ ¼
    
    def _rank_repositories(self, repos, paper_title, keywords, strategy="general"):
        """
        å¯¹æœç´¢ç»“æœæŒ‰ç›¸å…³æ€§æ’åºï¼Œè¿”å›æœ€ä½³åŒ¹é…
        æ”¹è¿›ç‰ˆï¼šæ›´ä¸¥æ ¼çš„è¯„åˆ†æ ‡å‡†ï¼Œé™ä½æ˜Ÿæ˜Ÿæ•°çš„æƒé‡
        """
        scored_repos = []
        title_lower = paper_title.lower()
        keywords_lower = [kw.lower() for kw in keywords]
        
        # ç‰¹åˆ«å…³æ³¨çš„æ¨¡å¼ï¼š
        # 1. å¸¸è§çš„è®ºæ–‡å®ç°å‘½åæ¨¡å¼ï¼ˆå¦‚"Paper-Pytorch", "Model-Implementation"ç­‰ï¼‰
        implementation_patterns = ['implementation', 'official', 'pytorch', 'tensorflow', 'code', 'paper']
        
        # 2. æ˜æ˜¾ä¸ç›¸å…³çš„é€šç”¨ä»“åº“
        generic_tools = ['howdoi', 'graphrag', 'xpra', 'wavelets', 'nboost', 'awesome-papers', 'robustvideo']
        
        # 3. æå–ä¸»è¦æ¦‚å¿µï¼ˆå†’å·å‰çš„éƒ¨åˆ†æˆ–ç¬¬ä¸€ä¸ªè¯ç»„ï¼‰
        primary_concept = ""
        if ':' in paper_title:
            primary_concept = paper_title.split(':')[0].strip().lower()
            primary_concept_words = primary_concept.split()
        else:
            # å–å‰3ä¸ªè¯ä½œä¸ºä¸»è¦æ¦‚å¿µ
            words = paper_title.split()
            primary_concept = ' '.join(words[:min(3, len(words))]).lower()
            primary_concept_words = words[:min(3, len(words))]
        
        for repo in repos:
            score = 0
            repo_name = repo.get('name', '').lower()
            repo_description = repo.get('description', '') or ''
            repo_description = repo_description.lower()
            
            # 0. é¢„å…ˆè¿‡æ»¤æ˜æ˜¾ä¸ç›¸å…³çš„å·¥å…·åº“
            if repo_name in generic_tools or any(tool in repo_name for tool in generic_tools):
                # é™¤éåç§°ä¸­æœ‰æ˜ç¡®çš„å…³é”®è¯åŒ¹é…ï¼Œå¦åˆ™è·³è¿‡é€šç”¨å·¥å…·
                if not any(kw.lower() in repo_name for kw in keywords_lower[:2]):
                    score -= 100  # ä¸¥é‡æƒ©ç½šï¼Œå®é™…ä¸Šæ’é™¤è¿™äº›ä»“åº“
                    scored_repos.append((score, repo))
                    continue
            
            # 1. ä»“åº“åä¸ä¸»è¦æ¦‚å¿µè¯åŒ¹é… (æƒé‡æœ€é«˜)
            concept_matches = 0
            for concept_word in primary_concept_words:
                if len(concept_word) > 2 and concept_word.lower() in repo_name:
                    concept_matches += 1
                    if concept_word.lower() == repo_name or repo_name.startswith(concept_word.lower()):
                        score += 15  # ç²¾ç¡®åŒ¹é…ç»™äºˆæ›´é«˜åˆ†æ•°
                    else:
                        score += 10
            
            # 2. ä»“åº“åä¸å…³é”®è¯åŒ¹é…
            exact_name_match = False
            for kw in keywords_lower:
                if kw in repo_name:
                    if kw == repo_name or repo_name.startswith(kw) or repo_name.endswith(kw):
                        score += 8  # ç²¾ç¡®åŒ¹é…æ›´é«˜åˆ†
                        exact_name_match = True
                    else:
                        score += 5
            
            # 3. ç‰¹åˆ«åŠ åˆ†ï¼šå®ç°æ¨¡å¼ (implementation, official, pytorchç­‰)
            impl_match = False
            for pattern in implementation_patterns:
                if pattern in repo_name:
                    score += 6
                    impl_match = True
                    break
                    
            # 4. æè¿°åŒ¹é…
            desc_matches = 0
            for kw in keywords_lower:
                if kw in repo_description:
                    score += 3
                    desc_matches += 1
            
            # æ£€æŸ¥æè¿°ä¸­æ˜¯å¦åŒ…å«"implementation"æˆ–"official"ç­‰æŒ‡ç¤ºè¯
            paper_indicators = ['implementation', 'official', 'code for', 'paper', 'reproduction', 'pytorch implementation']
            if any(pattern in repo_description for pattern in paper_indicators):
                score += 5
                # å¦‚æœåŒæ—¶åŒ…å«å…³é”®è¯ï¼Œç»™é¢å¤–åŠ åˆ†
                if desc_matches > 0:
                    score += 3
            
            # 5. ç¬¬ä¸€ä¸ªå…³é”®è¯çš„ç‰¹æ®ŠåŠ åˆ†ï¼ˆé€šå¸¸æ˜¯æ¨¡å‹åï¼‰
            if keywords_lower and keywords_lower[0] in repo_name:
                score += 7
                
            # 6. Starsæ•°é‡åŠ åˆ†ï¼ˆä½†æƒé‡é™ä½ï¼Œé¿å…è¢«é«˜æ˜Ÿä»“åº“è¯¯å¯¼ï¼‰
            stars = repo.get('stargazers_count', 0)
            if stars > 5000:
                # é«˜æ˜Ÿé¡¹ç›®æœ‰ç‰¹æ®Šå¤„ç†ï¼š
                # å¦‚æœæ˜¯é€šç”¨å·¥å…·ä½†å…³é”®è¯åŒ¹é…åº¦ä½ï¼Œå‡åˆ†è€Œä¸æ˜¯åŠ åˆ†
                if repo_name in generic_tools or any(pattern in repo_name for pattern in ['tool', 'utility', 'framework']):
                    if not exact_name_match and not impl_match and concept_matches == 0:
                        score -= 15  # ä¸¥å‰æƒ©ç½šæ˜æ˜¾ä¸ç›¸å…³çš„é«˜æ˜Ÿé¡¹ç›®
                elif concept_matches >= 1 or exact_name_match:
                    # å¦‚æœé«˜æ˜Ÿé¡¹ç›®æœ‰æ˜ç¡®çš„æ¦‚å¿µåŒ¹é…ï¼Œé€‚å½“åŠ åˆ†
                    score += min(1.5, stars / 10000)  # æœ€å¤šåŠ 1.5åˆ†ï¼Œå¤§å¹…é™ä½æƒé‡
                else:
                    # å…¶ä»–é«˜æ˜Ÿé¡¹ç›®å¾ˆå°çš„åŠ åˆ†
                    score += min(0.5, stars / 20000)  # æœ€å¤šåŠ 0.5åˆ†
            elif stars > 1000:
                score += min(stars / 2000, 1)  # æœ€å¤šåŠ 1åˆ†
            elif stars > 100:
                score += 0.5
            
            # 7. æœ€è¿‘æ›´æ–°åŠ åˆ†
            updated_at = repo.get('updated_at', '')
            if updated_at:
                try:
                    updated_date = datetime.strptime(updated_at, "%Y-%m-%dT%H:%M:%SZ")
                    days_since_update = (datetime.now() - updated_date).days
                    if days_since_update < 180:  # åŠå¹´å†…æ›´æ–°è¿‡
                        score += 1
                except:
                    pass
            
            # 8. æ ¹æ®æœç´¢ç­–ç•¥è°ƒæ•´é˜ˆå€¼
            min_score_threshold = {
                "exact": 18,    # ç²¾ç¡®æœç´¢è¦æ±‚æ›´é«˜åˆ†æ•°
                "context": 15,  # ä¸Šä¸‹æ–‡æœç´¢
                "multi": 12,    # å¤šå…³é”®è¯æœç´¢
                "single": 10,   # å•å…³é”®è¯æœç´¢
                "general": 8    # ä¸€èˆ¬æœç´¢
            }.get(strategy, 8)
            
            # 9. ç‰¹æ®Šæƒ©ç½šï¼šæ˜æ˜¾ä¸ç›¸å…³çš„ä»“åº“
            irrelevant_patterns = ['awesome', 'list', 'collection', 'tutorial', 'course', 'book', 'survey', 'api', 'howto']
            for pattern in irrelevant_patterns:
                if pattern in repo_name and not exact_name_match and concept_matches == 0:
                    score -= 8  # å‡åˆ†
            
            # 10. é¢å¤–çš„ç›¸å…³æ€§æ£€æŸ¥
            # è®¡ç®—ä»“åº“åå’Œè®ºæ–‡æ ‡é¢˜ä¹‹é—´çš„å•è¯é‡å åº¦
            repo_words = set(repo_name.split('-') + repo_name.split('_'))
            title_words = set(w.lower() for w in re.findall(r'\b[A-Za-z]+\b', paper_title) if len(w) > 2)
            overlap = len(repo_words.intersection(title_words))
            
            if overlap >= 2:
                score += 4  # æœ‰å¤šä¸ªè¯é‡å 
            elif overlap == 1:
                score += 2  # æœ‰ä¸€ä¸ªè¯é‡å 
            
            scored_repos.append((score, repo))
        
        # æŒ‰åˆ†æ•°æ’åºï¼Œè¿”å›æœ€é«˜åˆ†çš„ä»“åº“
        if scored_repos:
            scored_repos.sort(key=lambda x: x[0], reverse=True)
            best_score, best_repo = scored_repos[0]
            
            # ä½¿ç”¨ç­–ç•¥ç›¸å…³çš„é˜ˆå€¼
            min_threshold = {
                "exact": 18,
                "context": 15,
                "multi": 12, 
                "single": 10,
                "general": 8
            }.get(strategy, 8)
            
            print(f"    ğŸ“Š Best match score: {best_score:.1f} (threshold: {min_threshold})")
            
            if best_score < min_threshold:
                print(f"    âŒ Score too low, rejecting match")
                return None
                
            return best_repo
        
        return None
