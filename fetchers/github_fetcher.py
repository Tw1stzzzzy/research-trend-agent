import requests
import re
import time
from datetime import datetime

class GitHubFetcher:
    """
    åˆ©ç”¨ GitHub API æŸ¥è¯¢æŸä¸ª Repo çš„ stars æ•°é‡ä¸Žå¼€æºæ—¶é•¿ï¼ˆå¤©ï¼‰ã€‚
    """

    def __init__(self, github_token):
        self.headers = {"Authorization": f"token {github_token}"}

    def get_repo_stats(self, repo_url):
        """
        repo_url å½¢å¦‚ "https://github.com/owner/repo"
        è¿”å›žå­—å…¸ï¼š{'stars': int, 'forks': int, 'days_since_created': int}
        è‹¥æ— æ³•èŽ·å–åˆ™è¿”å›ž None
        """
        if not repo_url or 'github.com' not in repo_url:
            return None

        owner_repo = repo_url.replace("https://github.com/", "").strip("/")
        api_url = f"https://api.github.com/repos/{owner_repo}"
        response = requests.get(api_url, headers=self.headers)
        if response.status_code != 200:
            return None

        data = response.json()
        created_at = data.get('created_at')
        stars = data.get('stargazers_count', 0)
        forks = data.get('forks_count', 0)

        created_date = datetime.strptime(created_at, "%Y-%m-%dT%H:%M:%SZ")
        days_since_created = (datetime.now() - created_date).days

        return {
            'stars': stars,
            'forks': forks,
            'days_since_created': days_since_created
        }
    
    def extract_keywords(self, title):
        """
        ä»Žè®ºæ–‡æ ‡é¢˜ä¸­æå–å…³é”®è¯ï¼Œç”¨äºŽGitHubæœç´¢
        """
        # ç§»é™¤å¸¸è§åœç”¨è¯
        stopwords = {'via', 'for', 'with', 'on', 'in', 'the', 'a', 'an', 'and', 'or', 'but', 'of', 'to', 'from', 'by', 'at'}
        
        # æå–å¯èƒ½çš„æ¨¡åž‹åç§°ï¼ˆå¤§å†™å­—æ¯å¼€å¤´çš„è¿žç»­è¯ï¼‰
        model_names = re.findall(r'[A-Z][a-zA-Z]*(?:-[A-Z][a-zA-Z]*)*', title)
        
        # æå–é‡è¦çš„æŠ€æœ¯è¯æ±‡
        words = re.findall(r'\b[A-Za-z]+(?:-[A-Za-z]+)*\b', title)
        keywords = [word for word in words if word.lower() not in stopwords and len(word) > 2]
        
        # ä¼˜å…ˆè¿”å›žæ¨¡åž‹åç§°å’Œé‡è¦å…³é”®è¯
        all_keywords = model_names + keywords
        # åŽ»é‡å¹¶ä¿æŒé¡ºåº
        unique_keywords = []
        for kw in all_keywords:
            if kw not in unique_keywords:
                unique_keywords.append(kw)
        
        return unique_keywords[:5]  # è¿”å›žå‰5ä¸ªæœ€é‡è¦çš„å…³é”®è¯
    
    def search_paper_repository(self, paper_title):
        """
        åŸºäºŽè®ºæ–‡æ ‡é¢˜æœç´¢GitHubä»“åº“
        è¿”å›žæœ€åŒ¹é…çš„ä»“åº“URLå’Œç»Ÿè®¡ä¿¡æ¯ï¼Œå¦‚æžœæ²¡æ‰¾åˆ°è¿”å›žNone
        """
        print(f"    ðŸ” Searching GitHub for: {paper_title[:50]}...")
        
        keywords = self.extract_keywords(paper_title)
        if not keywords:
            return None
        
        # æž„é€ æœç´¢æŸ¥è¯¢ï¼šä¼˜å…ˆç”¨å‰2ä¸ªå…³é”®è¯
        primary_keywords = keywords[:2]
        query = f"{' '.join(primary_keywords)} language:python"
        
        # å¦‚æžœç¬¬ä¸€ä¸ªå…³é”®è¯çœ‹èµ·æ¥åƒæ¨¡åž‹åï¼ˆåŒ…å«å¤§å†™å’Œè¿žå­—ç¬¦ï¼‰ï¼Œä¼˜å…ˆæœç´¢
        if re.match(r'^[A-Z].*-.*[A-Z]', keywords[0]):
            query = f'"{keywords[0]}" language:python'
        
        return self._search_github_repos(query, paper_title, keywords)
    
    def _search_github_repos(self, query, paper_title, keywords):
        """
        æ‰§è¡ŒGitHubæœç´¢å¹¶è¿”å›žæœ€ä½³åŒ¹é…
        """
        api_url = "https://api.github.com/search/repositories"
        params = {
            'q': query,
            'sort': 'stars',
            'order': 'desc',
            'per_page': 10  # åªèŽ·å–å‰10ä¸ªç»“æžœ
        }
        
        try:
            response = requests.get(api_url, headers=self.headers, params=params)
            
            if response.status_code == 403:  # Rate limit
                print(f"    âš ï¸  GitHub API rate limit, waiting...")
                time.sleep(60)  # ç­‰å¾…1åˆ†é’Ÿ
                response = requests.get(api_url, headers=self.headers, params=params)
            
            if response.status_code != 200:
                print(f"    âŒ GitHub search failed: {response.status_code}")
                return None
            
            data = response.json()
            if not data.get('items'):
                print(f"    âŒ No repositories found")
                return None
            
            # å¯¹ç»“æžœè¿›è¡Œç›¸å…³æ€§æŽ’åº
            best_repo = self._rank_repositories(data['items'], paper_title, keywords)
            
            if best_repo:
                repo_url = best_repo['html_url']
                stats = self.get_repo_stats(repo_url)
                if stats:
                    print(f"    âœ… Found: {best_repo['name']} ({stats['stars']} â­)")
                    return {
                        'repo_url': repo_url,
                        'stats': stats
                    }
        
        except Exception as e:
            print(f"    âŒ GitHub search error: {e}")
        
        return None
    
    def _rank_repositories(self, repos, paper_title, keywords):
        """
        å¯¹æœç´¢ç»“æžœæŒ‰ç›¸å…³æ€§æŽ’åºï¼Œè¿”å›žæœ€ä½³åŒ¹é…
        """
        scored_repos = []
        title_lower = paper_title.lower()
        keywords_lower = [kw.lower() for kw in keywords]
        
        for repo in repos:
            score = 0
            repo_name = repo.get('name', '').lower()
            repo_description = repo.get('description', '') or ''
            repo_description = repo_description.lower()
            
            # 1. ä»“åº“ååŒ¹é… (æƒé‡æœ€é«˜)
            for kw in keywords_lower:
                if kw in repo_name:
                    score += 5
                    
            # 2. æè¿°åŒ¹é…
            for kw in keywords_lower:
                if kw in repo_description:
                    score += 2
            
            # 3. ç¬¬ä¸€ä¸ªå…³é”®è¯çš„ç²¾ç¡®åŒ¹é…åŠ åˆ†
            if keywords_lower and keywords_lower[0] in repo_name:
                score += 3
                
            # 4. Starsæ•°é‡åŠ åˆ†ï¼ˆå½’ä¸€åŒ–ï¼‰
            stars = repo.get('stargazers_count', 0)
            if stars > 100:
                score += min(stars / 100, 5)  # æœ€å¤šåŠ 5åˆ†
            elif stars > 10:
                score += 1
            
            # 5. æœ€è¿‘æ›´æ–°åŠ åˆ†
            updated_at = repo.get('updated_at', '')
            if updated_at:
                try:
                    updated_date = datetime.strptime(updated_at, "%Y-%m-%dT%H:%M:%SZ")
                    days_since_update = (datetime.now() - updated_date).days
                    if days_since_update < 365:  # ä¸€å¹´å†…æ›´æ–°è¿‡
                        score += 1
                except:
                    pass
            
            scored_repos.append((score, repo))
        
        # æŒ‰åˆ†æ•°æŽ’åºï¼Œè¿”å›žæœ€é«˜åˆ†çš„ä»“åº“
        if scored_repos:
            scored_repos.sort(key=lambda x: x[0], reverse=True)
            best_score, best_repo = scored_repos[0]
            
            # å¦‚æžœæœ€é«˜åˆ†å¤ªä½Žï¼Œè¯´æ˜ŽåŒ¹é…åº¦ä¸å¤Ÿ
            if best_score < 3:
                return None
                
            return best_repo
        
        return None
