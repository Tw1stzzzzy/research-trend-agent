import re
import json
from difflib import SequenceMatcher

def validate_and_clean_matches(scored_papers):
    """
    å¯¹åŒ¹é…ç»“æœè¿›è¡ŒéªŒè¯å’Œæ¸…ç†ï¼Œç¡®ä¿é«˜è´¨é‡çš„åŒ¹é…
    è¾“å…¥: scored_papersåˆ—è¡¨
    è¾“å‡º: æ¸…ç†åçš„scored_papersåˆ—è¡¨
    """
    print("ğŸ§¹ Validating and cleaning repository matches...")
    
    # 0. æ£€æŸ¥ç‰¹å®šçš„é”™è¯¯åŒ¹é…
    known_bad_matches = {
        "Frame Interpolation": ["google-research/frame-interpolation", "frame-interpolation"],
        "Robust 3D Shape": ["RobustVideoMatting", "robustmatting"],
        "Paint by Example": ["stable-diffusion", "controlnet"]
    }
    
    for paper in scored_papers:
        for keyword, bad_repos in known_bad_matches.items():
            if keyword in paper['title'] and paper.get('repo'):
                repo_name = paper['repo'].split('/')[-1].lower()
                full_repo_path = paper['repo'].replace("https://github.com/", "").lower()
                
                if repo_name in bad_repos or full_repo_path in bad_repos:
                    print(f"âš ï¸ Removing known bad match: {paper['title']} -> {paper['repo']}")
                    paper['repo'] = None
                    paper['stars'] = 0
    
    # 1. è¯†åˆ«è¯¯åŒ¹é…çš„æ¨¡å¼
    # ä¾‹å¦‚è®ºæ–‡A -> ä»“åº“Aï¼Œè®ºæ–‡B -> ä»“åº“A (åŒä¸€ä¸ªä»“åº“åŒ¹é…åˆ°å¤šä¸ªè®ºæ–‡)
    repo_to_papers = {}
    for paper in scored_papers:
        repo_url = paper.get('repo')
        if repo_url:
            if repo_url not in repo_to_papers:
                repo_to_papers[repo_url] = []
            repo_to_papers[repo_url].append(paper['title'])
    
    # æ£€æŸ¥é‡å¤åŒ¹é…çš„ä»“åº“
    duplicate_repos = {repo: papers for repo, papers in repo_to_papers.items() if len(papers) > 1}
    
    # 2. å¯¹äºæ¯ä¸ªé‡å¤åŒ¹é…çš„ä»“åº“ï¼Œä¿ç•™æœ€å¯èƒ½æ­£ç¡®çš„åŒ¹é…
    for repo_url, paper_titles in duplicate_repos.items():
        print(f"âš ï¸ Repository {repo_url} matched to multiple papers:")
        for title in paper_titles:
            print(f"  - {title}")
        
        # æå–ä»“åº“å
        repo_name = repo_url.split('/')[-1].lower() if repo_url else ""
        
        # è®¡ç®—æ¯ä¸ªè®ºæ–‡æ ‡é¢˜ä¸ä»“åº“åçš„ç›¸ä¼¼åº¦
        best_match = None
        best_score = 0
        
        for title in paper_titles:
            # æå–æ ‡é¢˜ä¸­çš„æ¨¡å‹åï¼ˆé€šå¸¸æ˜¯å†’å·å‰çš„éƒ¨åˆ†ï¼‰
            model_name = title.split(':')[0].strip().lower() if ':' in title else title.split()[0].lower()
            
            # è®¡ç®—ç›¸ä¼¼åº¦ (ä½¿ç”¨æœ€é•¿å…¬å…±å­åºåˆ—)
            similarity = SequenceMatcher(None, model_name, repo_name).ratio()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ç²¾ç¡®çš„å…³é”®è¯åŒ¹é…
            title_words = re.findall(r'\b[a-z0-9]{3,}\b', title.lower())
            word_match = any(word in repo_name for word in title_words)
            
            # è¯åŒ¹é…æ¯”ä¸€èˆ¬ç›¸ä¼¼åº¦æ›´é‡è¦
            if word_match:
                similarity += 0.3
                
            if similarity > best_score:
                best_score = similarity
                best_match = title
        
        print(f"  âœ… Best match: {best_match} (score: {best_score:.2f})")
        
        # æ¸…é™¤å…¶ä»–è®ºæ–‡çš„è¯¥ä»“åº“åŒ¹é…
        for paper in scored_papers:
            if paper['title'] in paper_titles and paper['title'] != best_match:
                print(f"  âŒ Removing match from: {paper['title']}")
                paper['repo'] = None
                paper['stars'] = 0
    
    # 3. æ£€æŸ¥å¯ç–‘çš„åŒ¹é… (å¦‚åŸºäºå•ä¸€é€šç”¨è¯æ±‡çš„åŒ¹é…)
    suspicious_keywords = ['tool', 'utility', 'framework', 'awesome', 'list', 'collection', 'tutorial']
    for paper in scored_papers:
        repo_url = paper.get('repo')
        if repo_url:
            repo_name = repo_url.split('/')[-1].lower()
            title_words = re.findall(r'\b[a-z0-9]{3,}\b', paper['title'].lower())
            
            # æ£€æŸ¥ä»“åº“åæ˜¯å¦åªåŒ¹é…äº†æ ‡é¢˜ä¸­çš„é€šç”¨è¯
            matches = [word for word in title_words if word in repo_name]
            
            # 1. å¦‚æœä»“åº“ååŒ…å«å¯ç–‘å…³é”®è¯ä½†ä¸åŒ…å«è®ºæ–‡ç‰¹å®šè¯æ±‡
            if any(keyword in repo_name for keyword in suspicious_keywords):
                specific_match = False
                for word in title_words:
                    # è·³è¿‡å¸¸è§é€šç”¨è¯
                    if word in ['model', 'deep', 'learning', 'neural', 'network', 'using', 'with', 'based']:
                        continue
                    if word in repo_name:
                        specific_match = True
                        break
                        
                if not specific_match and paper.get('stars', 0) > 1000:
                    print(f"âš ï¸ Suspicious match - generic high-star repo: {paper['title']} -> {repo_url}")
                    paper['repo'] = None
                    paper['stars'] = 0
            
            # 2. å¦‚æœä»“åº“åç§°å¾ˆé•¿ï¼Œä½†ä¸è®ºæ–‡æ ‡é¢˜æ²¡æœ‰æ˜¾è‘—é‡å 
            elif len(repo_name) > 15:
                overlap_ratio = len(matches) / len(title_words) if title_words else 0
                if overlap_ratio < 0.2 and paper.get('stars', 0) > 2000:
                    print(f"âš ï¸ Suspicious match - low overlap ratio: {paper['title']} -> {repo_url}")
                    paper['repo'] = None
                    paper['stars'] = 0
    
    # 4. æ£€æŸ¥æ¨¡å‹åç§°ä¸ä»“åº“åç§°çš„åŒ¹é…
    for paper in scored_papers:
        if not paper.get('repo'):
            continue
            
        title = paper['title']
        repo_url = paper['repo']
        repo_name = repo_url.split('/')[-1].lower() if repo_url else ""
        
        # æå–æ¨¡å‹åç§° (é€šå¸¸æ˜¯å†’å·å‰çš„éƒ¨åˆ†æˆ–æ ‡é¢˜çš„ç¬¬ä¸€ä¸ªè¯)
        model_name = ""
        if ':' in title:
            model_name = title.split(':')[0].strip()
        else:
            # å‡è®¾ç¬¬ä¸€ä¸ªè¯å¯èƒ½æ˜¯æ¨¡å‹å
            model_name = title.split()[0]
        
        # æ£€æŸ¥æ¨¡å‹åæ˜¯å¦åœ¨ä»“åº“åä¸­
        model_in_repo = False
        model_words = model_name.lower().split()
        
        for word in model_words:
            if len(word) > 2 and word.lower() in repo_name:
                model_in_repo = True
                break
        
        # å¦‚æœæ¨¡å‹åä¸åœ¨ä»“åº“åä¸­ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªé«˜æ˜Ÿä»“åº“ï¼Œåˆ™å¯ç–‘
        if not model_in_repo and paper.get('stars', 0) > 3000:
            # å†æ¬¡éªŒè¯æ˜¯å¦æœ‰å…¶ä»–å¼ºåŒ¹é…ç‰¹å¾
            title_words = set(re.findall(r'\b[a-z0-9]{3,}\b', title.lower()))
            repo_words = set(re.findall(r'\b[a-z0-9]{3,}\b', repo_name))
            
            # è®¡ç®—ä¸¤è€…çš„è¯æ±‡é‡å 
            overlap = title_words.intersection(repo_words)
            
            # å¦‚æœé‡å è¯æ±‡å°‘äº2ä¸ªï¼Œä¸”éƒ½æ˜¯å¸¸è§è¯ï¼Œåˆ™å¯ç–‘
            common_words = {'deep', 'learning', 'neural', 'network', 'model', 'gan', 'net', 'transformer'}
            meaningful_overlap = [word for word in overlap if word not in common_words]
            
            if len(meaningful_overlap) < 1:
                print(f"âš ï¸ Suspicious match - model name not in repo: {title} -> {repo_url}")
                paper['repo'] = None
                paper['stars'] = 0
    
    print(f"âœ… Validation complete. {sum(1 for p in scored_papers if p.get('repo'))} valid matches remain.")
    return scored_papers 