import os
import yaml
import json
import time

from fetchers.openreview_fetcher import OpenReviewFetcher
from fetchers.acl_fetcher import ACLFetcher
from fetchers.cvf_fetcher import CVFFetcher
from fetchers.github_fetcher import GitHubFetcher
from fetchers.pwcode_fetcher import PWCodeFetcher

from processors.filter_and_summarize import process_papers
from processors.scoring import calculate_score
from processors.trend_analyzer import analyze_trends
from processors.report_generator import generate_report
from processors.paper_processor import validate_and_clean_matches

import requests
from datetime import datetime

# ‚îÄ‚îÄ 1. ËØªÂèñÈÖçÁΩÆ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with open("configs/config.yaml", "r") as f:
    config = yaml.safe_load(f)

# OpenReview ÊãâÂèñËµ∑ÂßãÊó•Êúü
since_date = config['fetch']['since_date']

# API Keys
PWC_API_KEY = config['paperswithcode']['api_key']
GITHUB_TOKEN = config['github']['token']
SLACK_WEBHOOK = config['slack'].get('webhook_url', "")

# ÂàùÂßãÂåñ Fetchers
pwcode_fetcher = PWCodeFetcher(PWC_API_KEY)
github_fetcher = GitHubFetcher(GITHUB_TOKEN)

# ‰ºöËÆÆÂàóË°®ÂíåÂØπÂ∫î Fetcher ÂàùÂßãÂåñ
# OpenReview ‰ºöËÆÆ ID (‰ΩøÁî®2023Âπ¥Êï∞ÊçÆËøõË°åÊµãËØï)
openreview_confs = [
    "ICLR.cc/2023/Conference",
    # "NeurIPS.cc/2023/Conference",  # ÂÖàÂè™ÊµãËØï‰∏Ä‰∏™‰ºöËÆÆ
    # "ICML.cc/2023/Conference"
]

# CVF ‰ºöËÆÆ URL Âíå Venue ÂêçÁß∞ (‰ΩøÁî®2023Âπ¥Êï∞ÊçÆ)
cvf_confs = [
    ("https://openaccess.thecvf.com/CVPR2023", "CVPR"),
    # ("https://openaccess.thecvf.com/ECCV2023", "ECCV")  # ÂÖàÂè™ÊµãËØï‰∏Ä‰∏™‰ºöËÆÆ
]

# ACL ‰ºöËÆÆËÆæÁΩÆ
acl_year = "2024"
acl_conf = "ACL"

# ‚îÄ‚îÄ 2. ÊãâÂèñÂπ∂ÂêàÂπ∂ÊâÄÊúâ‰ºöËÆÆËÆ∫Êñá ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
all_papers = []

# 2.1 OpenReview ÈÉ®ÂàÜ (ÊöÇÊó∂Ë∑≥ËøáÔºåÈÅøÂÖçAPIÈôêÂà∂)
print("‚è≠Ô∏è  OpenReview fetching temporarily skipped (API rate limiting issues)")
# for conf in openreview_confs:
#     fetcher = OpenReviewFetcher(conf)
#     papers = fetcher.fetch_papers(since_date)
#     all_papers.extend(papers)

# 2.2 CVF ÈÉ®ÂàÜ (Â∑≤ÂèëË°®ËÆ∫Êñá)
for url, venue in cvf_confs:
    fetcher = CVFFetcher(url, venue)
    papers = fetcher.fetch_papers(max_papers=300)  # Â¢ûÂä†Ëé∑ÂèñÊï∞Èáè‰ª•ÊâæÂà∞Êõ¥Â§öÊúâGitHubÁöÑËÆ∫Êñá
    all_papers.extend(papers)
    print(f"üìö Fetched {len(papers)} papers from {venue}")

# 2.3 ACL ÈÉ®ÂàÜ (ÊöÇÊó∂Ê≥®ÈáäÔºåURLÂèØËÉΩÊúâÈóÆÈ¢ò)
# acl_fetcher = ACLFetcher(year=acl_year, conference=acl_conf)
# acl_papers = acl_fetcher.fetch_papers()
# all_papers.extend(acl_papers)
print("ACL fetching temporarily skipped (URL needs fixing)")

# 2.4 Save raw fetched data (optional)
os.makedirs("output", exist_ok=True)
with open("output/raw_papers.json", "w", encoding="utf-8") as f:
    json.dump(all_papers, f, indent=2)

print(f"‚úÖ Paper fetching completed: {len(all_papers)} papers collected")

# ‚îÄ‚îÄ 3. ÂÖ≥ÈîÆËØçÁ≠õÈÄâ & ÊëòË¶ÅÁ≤æÁÆÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
filtered_papers = process_papers(all_papers)
with open("output/filtered_papers.json", "w", encoding="utf-8") as f:
    json.dump(filtered_papers, f, indent=2)

print(f"üîç Keyword filtering completed: {len(filtered_papers)} papers remain")

# Ê≠•È™§ 3ÔºöËÆ°ÁÆóÊØèÁØáËÆ∫ÊñáÁöÑÂàÜÊï∞
print("\nüìä Calculating paper scores...")
scored_papers = calculate_score(filtered_papers, github_fetcher, pwcode_fetcher)

# Ê≠•È™§ 3.5ÔºöÈ™åËØÅÂíåÊ∏ÖÁêÜÂåπÈÖçÁªìÊûúÔºåÊèêÈ´òÂåπÈÖçË¥®Èáè
print("\nüßπ Validating and cleaning repository matches...")
scored_papers = validate_and_clean_matches(scored_papers)

# ‰øùÂ≠òÁªìÊûú
with open("output/scored_papers.json", "w", encoding="utf-8") as f:
    json.dump(scored_papers, f, ensure_ascii=False, indent=2)

# ‚îÄ‚îÄ 5. Ë∂ãÂäøÁªüËÆ° & Êä•ÂëäÁîüÊàê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
stats = analyze_trends("output/scored_papers.json")
report_text = generate_report("output/scored_papers.json", "output")
print("üìÑ Trend report generated successfully ‚Üí output/report.md")

# ‚îÄ‚îÄ 6. Slack Êé®ÈÄÅÔºàËã•ÈÖçÁΩÆ‰∫Ü webhookÔºâ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if SLACK_WEBHOOK and SLACK_WEBHOOK != "your-slack-webhook-here":
    try:
        # ÂáÜÂ§áSlackÊ∂àÊÅØÂÜÖÂÆπ
        slack_summary = f"üìã *AI Research Trend Report ({datetime.now().strftime('%Y-%m-%d')})*\n\n"
        slack_summary += f"‚ú® *Total Papers*: {len(scored_papers)} papers analyzed\n"
        
        # ÊåâÊòüÊòüÊï∞ÊéíÂ∫è
        top_papers = sorted(scored_papers, key=lambda x: x.get('stars', 0), reverse=True)
        # ËøáÊª§Âè™ÊòæÁ§∫ÊòüÊòüÊï∞Ë∂ÖËøá500ÁöÑ‰ªìÂ∫ì
        high_star_papers = [p for p in top_papers if p.get('stars', 0) >= 500]
        
        if not high_star_papers:
            slack_summary += "\n*No papers with repositories having 500+ stars were found.*"
        else:
            slack_summary += f"\n*Top {len(high_star_papers[:5])} Recommended Papers*:"
        
        for i, paper in enumerate(high_star_papers[:5], 1):
            title = paper['title']
            # Êô∫ËÉΩÊà™Êñ≠Ôºö‰ºòÂÖà‰øùÁïôÂÆåÊï¥ÂçïËØçÔºåÊúÄÂ§ß80Â≠óÁ¨¶
            if len(title) > 80:
                # Âú®ÂçïËØçËæπÁïåÊà™Êñ≠
                words = title.split()
                truncated = ""
                for word in words:
                    if len(truncated + word) > 75:  # Áïô5‰∏™Â≠óÁ¨¶Áªô"..."
                        break
                    truncated += word + " "
                title = truncated.strip() + "..."
            
            authors = paper.get('authors', [])
            first_author = authors[0] if authors else "Unknown"
            author_text = f"{first_author} et al." if len(authors) > 1 else first_author
            
            repo_url = paper.get('repo', '')
            stars = paper.get('stars', 0)
            
            slack_summary += f"\n{i}. *{title}*"
            slack_summary += f"\n   Authors: {author_text} | Venue: {paper.get('venue', 'N/A')}"
            slack_summary += f"\n   *Repository*: {repo_url} ({stars} ‚≠ê)"
        
        slack_summary += f"\n\nFull detailed report: output/report.md"
        
        payload = {
            "text": slack_summary,
            "username": "Research Agent",
            "icon_emoji": ":robot_face:"
        }
        
        resp = requests.post(SLACK_WEBHOOK, json=payload)
        if resp.status_code == 200:
            print("üì± Slack notification sent successfully")
        else:
            print(f"‚ùå Slack notification failed: {resp.status_code} - {resp.text}")
    except Exception as e:
        print(f"‚ùå Slack notification error: {str(e)}")
else:
    print("‚è≠Ô∏è  Slack webhook not configured, skipping notification")

print("üéâ Research Agent execution completed successfully!")
