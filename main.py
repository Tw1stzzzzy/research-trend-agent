import os
import yaml
import json
import time

from fetchers.openreview_fetcher import OpenReviewFetcher
from fetchers.acl_fetcher import ACLFetcher
from fetchers.cvf_fetcher import CVFFetcher
from fetchers.pwcode_fetcher import PWCodeFetcher
from fetchers.github_fetcher import GitHubFetcher

from processors.filter_and_summarize import process_papers
from processors.scoring import calculate_score
from processors.trend_analyzer import analyze_trends
from processors.report_generator import generate_report

import requests
from datetime import datetime

# ‚îÄ‚îÄ 1. ËØªÂèñÈÖçÁΩÆ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
with open("configs/config.yaml", "r") as f:
    config = yaml.safe_load(f)

# OpenReview ÊãâÂèñËµ∑ÂßãÊó•Êúü
since_date = config['fetch']['since_date']

# API Keys
PWC_API_KEY = config['paperswithcode']['api_key']
GITHUB_TOKEN = config['github']['token']
SLACK_WEBHOOK = config['slack'].get('webhook_url', "")

# ÂàùÂßãÂåñ Fetchers
pwcode_fetcher = PWCodeFetcher(PWC_API_KEY)
github_fetcher = GitHubFetcher(GITHUB_TOKEN)

# ‰ºöËÆÆÂàóË°®ÂíåÂØπÂ∫î Fetcher ÂàùÂßãÂåñ
# OpenReview ‰ºöËÆÆ ID (‰ΩøÁî®2023Âπ¥Êï∞ÊçÆËøõË°åÊµãËØï)
openreview_confs = [
    "ICLR.cc/2023/Conference",
    # "NeurIPS.cc/2023/Conference",  # ÂÖàÂè™ÊµãËØï‰∏Ä‰∏™‰ºöËÆÆ
    # "ICML.cc/2023/Conference"
]

# CVF ‰ºöËÆÆ URL Âíå Venue ÂêçÁß∞ (‰ΩøÁî®2023Âπ¥Êï∞ÊçÆ)
cvf_confs = [
    ("https://openaccess.thecvf.com/CVPR2023", "CVPR"),
    # ("https://openaccess.thecvf.com/ECCV2023", "ECCV")  # ÂÖàÂè™ÊµãËØï‰∏Ä‰∏™‰ºöËÆÆ
]

# ACL ‰ºöËÆÆËÆæÁΩÆ
acl_year = "2024"
acl_conf = "ACL"

# ‚îÄ‚îÄ 2. ÊãâÂèñÂπ∂ÂêàÂπ∂ÊâÄÊúâ‰ºöËÆÆËÆ∫Êñá ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
all_papers = []

# 2.1 OpenReview ÈÉ®ÂàÜ (ÊöÇÊó∂Ë∑≥ËøáÔºåÈÅøÂÖçAPIÈôêÂà∂)
print("‚è≠Ô∏è  OpenReview fetching temporarily skipped (API rate limiting issues)")
# for conf in openreview_confs:
#     fetcher = OpenReviewFetcher(conf)
#     papers = fetcher.fetch_papers(since_date)
#     all_papers.extend(papers)

# 2.2 CVF ÈÉ®ÂàÜ (Â∑≤ÂèëË°®ËÆ∫Êñá)
for url, venue in cvf_confs:
    fetcher = CVFFetcher(url, venue)
    papers = fetcher.fetch_papers(max_papers=300)  # Â¢ûÂä†Ëé∑ÂèñÊï∞Èáè‰ª•ÊâæÂà∞Êõ¥Â§öÊúâGitHubÁöÑËÆ∫Êñá
    all_papers.extend(papers)
    print(f"üìö Fetched {len(papers)} papers from {venue}")

# 2.3 ACL ÈÉ®ÂàÜ (ÊöÇÊó∂Ê≥®ÈáäÔºåURLÂèØËÉΩÊúâÈóÆÈ¢ò)
# acl_fetcher = ACLFetcher(year=acl_year, conference=acl_conf)
# acl_papers = acl_fetcher.fetch_papers()
# all_papers.extend(acl_papers)
print("ACL fetching temporarily skipped (URL needs fixing)")

# 2.4 Save raw fetched data (optional)
os.makedirs("output", exist_ok=True)
with open("output/raw_papers.json", "w", encoding="utf-8") as f:
    json.dump(all_papers, f, indent=2)

print(f"‚úÖ Paper fetching completed: {len(all_papers)} papers collected")

# ‚îÄ‚îÄ 3. ÂÖ≥ÈîÆËØçÁ≠õÈÄâ & ÊëòË¶ÅÁ≤æÁÆÄ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
filtered_papers = process_papers(all_papers)
with open("output/filtered_papers.json", "w", encoding="utf-8") as f:
    json.dump(filtered_papers, f, indent=2)

print(f"üîç Keyword filtering completed: {len(filtered_papers)} papers remain")

# ‚îÄ‚îÄ 4. ËÆ§ÂèØÂ∫¶ÊâìÂàÜ & GitHubÁ≠õÈÄâ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
scored_results = []
skipped_no_github = 0

print("üîç Starting recognition scoring (GitHub repos required)...")
if PWC_API_KEY == "your-pwc-api-key-here" or not PWC_API_KEY:
    print("‚ö†Ô∏è  PapersWithCode API not configured, trying direct GitHub search")

for i, paper in enumerate(filtered_papers, 1):
    title = paper['title']
    print(f"  üìã Processing {i}/{len(filtered_papers)}: {title[:50]}...")
    
    # 4.1 Êü•ËØ¢ PapersWithCode
    pwc_info = None
    if PWC_API_KEY and PWC_API_KEY != "your-pwc-api-key-here":
        pwc_info = pwcode_fetcher.search_paper(title)
    
    is_pwcode = True if pwc_info else False
    repo_url = pwc_info['repo_url'] if pwc_info else None

    # 4.2 Â¶ÇÊûúÊ≤°Êúâ‰ªéPWCÊâæÂà∞ÔºåÂ∞ùËØïÁõ¥Êé•ÊêúÁ¥¢GitHubÔºàÂü∫‰∫éËÆ∫ÊñáÊ†áÈ¢òÔºâ
    github_result = None
    if not repo_url:
        github_result = github_fetcher.search_paper_repository(title)
        if github_result:
            repo_url = github_result['repo_url']
            github_stats = github_result['stats']
        
    # 4.3 Êü•ËØ¢ GitHub Repo StatsÔºàÂ¶ÇÊûúËøòÊ≤°ÊúâÁªüËÆ°‰ø°ÊÅØÔºâ
    if repo_url and not github_result:
        github_stats = github_fetcher.get_repo_stats(repo_url)
    elif github_result:
        github_stats = github_result['stats']
    else:
        github_stats = None
        
    stars = github_stats['stars'] if github_stats else None
    days_open = github_stats['days_since_created'] if github_stats else None

    # 4.4 Âè™‰øùÁïôÊúâGitHub‰ªìÂ∫ìÁöÑËÆ∫Êñá
    if not repo_url or not github_stats:
        print(f"    ‚ùå No GitHub repo found, skipping paper")
        skipped_no_github += 1
        continue
        
    # 4.5 ËÆ°ÁÆóÂàÜÊï∞ÔºàÂè™ÊúâÊúâGitHubÁöÑËÆ∫ÊñáÊâç‰ºöÂà∞ËøôÈáåÔºâ
    score = calculate_score(is_pwcode, stars, days_open)
    
    print(f"    ‚úÖ Found repo with {stars} stars, score: {score:.1f}")
    
    scored_results.append({
        'title': title,
        'authors': paper.get('authors', []),
        'summary': paper.get('summary', ""),
        'pdf_url': paper.get('pdf_url', ""),
        'venue': paper.get('venue', ""),
        'repo': repo_url,
        'stars': stars,
        'days_since_created': days_open,
        'score': score
    })

# ‰øùÂ≠òËØÑÂàÜÁªìÊûú
with open("output/scored_papers.json", "w", encoding="utf-8") as f:
    json.dump(scored_results, f, indent=2)

print(f"üìä Recognition scoring completed: {len(scored_results)} papers with GitHub repos found")
print(f"‚ö†Ô∏è  Skipped {skipped_no_github} papers without GitHub repositories")

# ‚îÄ‚îÄ 5. Ë∂ãÂäøÁªüËÆ° & Êä•ÂëäÁîüÊàê ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
stats = analyze_trends("output/scored_papers.json")
report_text = generate_report("output/scored_papers.json", "output")
print("üìÑ Trend report generated successfully ‚Üí output/report.md")

# ‚îÄ‚îÄ 6. Slack Êé®ÈÄÅÔºàËã•ÈÖçÁΩÆ‰∫Ü webhookÔºâ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if SLACK_WEBHOOK:
    try:
        # ÂàõÂª∫‰∏∞ÂØå‰ΩÜÊ∏ÖÊô∞ÁöÑSlackÊ∂àÊÅØ
        date_str = datetime.now().strftime('%Y-%m-%d')
        slack_summary = f"""*AI Research Trend Report* ({date_str})

*Summary Statistics:*
‚Ä¢ Total Papers: {stats.get('total_papers', 0)}
‚Ä¢ Open Source: {stats.get('open_source_count', 0)} ({stats.get('open_source_count', 0)/max(stats.get('total_papers', 1), 1)*100:.1f}%)
‚Ä¢ Average Score: {stats.get('avg_score', 0):.1f}/5.0

*Hot Research Topics:*"""
        
        # Ê∑ªÂä†ÂÖ≥ÈîÆËØçÂàÜÂ∏É
        if stats.get('keyword_counts'):
            sorted_keywords = sorted(stats['keyword_counts'].items(), key=lambda x: x[1], reverse=True)
            for kw, cnt in sorted_keywords[:3]:
                if cnt > 0:
                    percentage = (cnt / stats.get('total_papers', 1)) * 100
                    slack_summary += f"\n‚Ä¢ {kw.title()}: {cnt} papers ({percentage:.1f}%)"
        
        # Ê∑ªÂä†Êé®ËçêËÆ∫ÊñáÔºàÂâç3ÁØáÈ´òÂàÜËÆ∫ÊñáÔºâ
        with open("output/scored_papers.json", "r") as f:
            scored_papers = json.load(f)
        
        top_papers = sorted(scored_papers, key=lambda x: x['score'], reverse=True)[:3]
        
        slack_summary += "\n\n*Top Recommended Papers:*"
        for i, paper in enumerate(top_papers, 1):
            title = paper['title']
            # Êô∫ËÉΩÊà™Êñ≠Ôºö‰ºòÂÖà‰øùÁïôÂÆåÊï¥ÂçïËØçÔºåÊúÄÂ§ß100Â≠óÁ¨¶
            if len(title) > 100:
                # Âú®ÂçïËØçËæπÁïåÊà™Êñ≠
                words = title.split()
                truncated = ""
                for word in words:
                    if len(truncated + word) > 95:  # Áïô5‰∏™Â≠óÁ¨¶Áªô"..."
                        break
                    truncated += word + " "
                title = truncated.strip() + "..."
            
            authors = paper.get('authors', [])
            first_author = authors[0] if authors else "Unknown"
            author_text = f"{first_author} et al." if len(authors) > 1 else first_author
            
            slack_summary += f"\n{i}. *{title}*"
            slack_summary += f"\n   Authors: {author_text} | Venue: {paper.get('venue', 'N/A')} | ‚≠ê {paper.get('stars', 0)} stars"
        
        slack_summary += f"\n\nFull detailed report: output/report.md"
        
        payload = {
            "text": slack_summary,
            "username": "Research Agent",
            "icon_emoji": ":robot_face:"
        }
        
        resp = requests.post(SLACK_WEBHOOK, json=payload)
        if resp.status_code == 200:
            print("üì± Slack notification sent successfully")
        else:
            print(f"‚ùå Slack notification failed: {resp.status_code} - {resp.text}")
    except Exception as e:
        print(f"‚ùå Slack notification error: {str(e)}")
else:
    print("‚è≠Ô∏è  Slack webhook not configured, skipping notification")

print("üéâ Research Agent execution completed successfully!")
